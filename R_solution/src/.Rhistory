q()
install.packages("e1071")
install.packages("DESeq")
R.version
R.version
install.packages("DESeq")
install.packages("DESeq2")
source("https://bioconductor.org/biocLite.R")
biocLite("DESeq2")
source("https://bioconductor.org/biocLite.R")
install.packages("data.table")
install.packages("forecast")
install.packages("foreach")
install.packages("doParallel")
install.packages("iterators")
install.packages("~/R/Packages/doParallel_1.0.10.tar.gz", repos = NULL, type="source")
library(tibble)
random_string_column <- function(n) {
stringi::stri_rand_strings(n = n, length = 8)
}
random_data_frame <- function(n) tibble(
col1 = random_string_column(n),
col2 = random_string_column(n)
)
data <- random_data_frame(10^5)
data <- random_data_frame(10^6)
time(setkey(data_table, col1))
n <- 1:1e6
system.time(for (i in 1:n) sqrt(x)) / length(n)
system.time(for (i in 1:n) x ^ 0.5) / length(n)
n <- 1:1e6
system.time(for (i in 1:n) sqrt(i)) / length(n)
system.time(for (i in 1:n) i ^ 0.5) / length(n)
n <- 1:1e10
system.time(for (i in 1:n) sqrt(i)) / length(n)
system.time(for (i in 1:n) i ^ 0.5) / length(n)
n <- 1:1e13
system.time(for (i in 1:n) sqrt(i))
system.time(for (i in 1:n) i ^ 0.5)
DF = data.frame(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
DT = data.table(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
install.packages("data.table")
DF = data.frame(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
DT = data.table(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
library("data.table", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.4")
DF = data.frame(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
DT = data.table(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
DT
DF
setkey(DT,x)
DT
DT = data.table(x=rep(c("b","a","c"),each=3), y=c(1,3,6), v=1:9)
setkey(DT,x)
DT
library(data.table)
DF = data.frame(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
DT = data.table(x=rep(c("b","a","c"),each=3), y=c(1,3,6), v=1:9)
DT
setkey(DT,x)
DT
install.packages("sets")
library((sets))
library((sets)
library(sets)
set(seq(1,100,1))
a = set(seq(1,100,1))
is.element(1,a)
a
seq(1,100,1)
install.packages("hash")
library(hash)
h <- hash( keys=letters, values=1:26 )
h
h['q']
DT
DT['b']
DT['d']
setwd("~/PA_course/Parallel-Processing_backup")
library(data.table)
data =  read.table("../data/twitter_combined.txt", sep = " ")
dt = data.table(data)
dt
setwd("~/PA_course/Parallel-Processing_backup")
library(data.table)
data =  read.table("../data/twitter_combined.txt", sep = " ")
dt = data.table(data)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
library(data.table)
data =  read.table("../data/twitter_combined.txt", sep = " ")
dt = data.table(data)
dt
setkey(dt,V1)
dt
dt[1,]
dt[12,]
dt['12',]
dt["12",]
DT = data.table(A=5:1,B=letters[5:1])
DT # before
setkey(DT,A)
DT
DT[1,]
DF = data.frame(x=rep(c("a","b","c"),each=3), y=c(1,3,6), v=1:9)
DT = data.table(x=rep(c("b","a","c"),each=3), y=c(1,3,6), v=1:9)
DT
setkey(DT,x)
DT
DT[a,]
DT['a',]
setkey(DT,y)
DT['a',]
DT
DT[1,]
DT['1',]
key(DT)
DT['y'==1,]
DT[y==1,]
library(data.table)
data =  read.table("../data/twitter_combined.txt", sep = " ")
dt = data.table(data)
setkey(dt,V1)
score = 0
for (i in 1:ncol(data)) {
row = unlist(data[,i])
part = dt[V1==row[2],]
if (i%%20000==0) {cat(i,"\n")}
# if (length(indexes) > 0) {
#   for (index in indexes) {
#     if (data[2,index] == row[1] ) {
#       score = score +1
#       break
#     }
#   }
# }
}
score = 0
for (i in 1:ncol(data)) {
row = unlist(data[,i])
part = dt[V1==row[2],]
if (i%%20000==0) {cat(i,"\n")}
# if (length(indexes) > 0) {
#   for (index in indexes) {
#     if (data[2,index] == row[1] ) {
#       score = score +1
#       break
#     }
#   }
# }
}
score = 0
for (i in 1:nrow(data)) {
row = unlist(data[i,])
part = dt[V1==row[2],]
if (i%%20000==0) {cat(i,"\n")}
# if (length(indexes) > 0) {
#   for (index in indexes) {
#     if (data[2,index] == row[1] ) {
#       score = score +1
#       break
#     }
#   }
# }
}
install.packages("gtools")
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
data =  read.table("../data/twitter_combined.txt", sep = " ")
dim(data)
data = data[order(data[,1]),]
dim(data)
head(data)
data =  read.table("../data/twitter_combined.txt", sep = " ")
data[3346,]
head(data)
ncol(data)
data =  read.table("../data/twitter_combined.txt", sep = " ")
data = data[order(data[,1]),]
data= t(data.matrix(data))
ncol(data)
data =  read.table("../data/twitter_combined.txt", sep = " ")
data = data[order(data[,1]),]
ncol(data)
source("bin_search.R")
arr = seq(1,100,1)
bin_search(arr,1,100,2)
arr
source("bin_search.R")
arr = seq(1,100,1)
bin_search(arr,1,100,2)
source("bin_search.R")
arr = seq(1,100,1)
bin_search(arr,1,100,2)
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
arr
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
source("bin_search.R")
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
source("bin_search.R")
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
source("bin_search.R")
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
source("bin_search.R")
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
source("bin_search.R")
n= 5
arr = seq(1,n,1)
bin_search(arr,1,n,2)
n= 6
arr = seq(1,n,1)
bin_search(arr,1,n,2)
cat(arr)
source("bin_search.R")
n= 6
arr = seq(1,n,1)
bin_search(arr,1,n,2)
source("bin_search.R")
n= 6
arr = seq(1,n,1)
bin_search(arr,1,n,2)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
source("bin_search.R")
n= 3
arr = seq(1,n,1)
bin_search(arr,1,n,2)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
source("bin_search.R")
n= 4
arr = seq(1,n,1)
bin_search(arr,1,n,2)
setwd("~/PA_course/Parallel-Processing_backup/R_solution/src")
source("bin_search.R")
n= 4
arr = seq(1,n,1)
bin_search(arr,1,n,2)
n= 7
arr = seq(1,n,1)
bin_search(arr,1,n,2)
source("bin_search.R")
n= 7
arr = seq(1,n,1)
bin_search(arr,1,n,2)
source("bin_search.R")
// test the bin
n= 7
arr = seq(1,n,1)
bin_search(arr,1,n,2)
data =  read.table("../data/twitter_combined.txt", sep = " ")
data = data[order(data[,1]),]
col1= unlist(data[,1])
data = data.matrix(data)
score = 0
N = nrow(data)
N
for (i in 1:nrow(data)) {
row = unlist(data[i,])
index = bin_search(col1,1,N,row[2])
# indexes = seq(1,100,1)
if (i%%20000==0) {cat(i,"\n")}
# if (length(indexes) > 0) {
#   for (index in indexes) {
#     if (data[2,index] == row[1] ) {
#     score = score +1
#     break
#     }
#   }
# }
}
score = 0
N = nrow(data)
for (i in 1:nrow(data)) {
row = unlist(data[i,])
index = bin_search(col1,1,N,row[2])
if (i%%20000==0) {cat(i,"\n")}
if (index != -1) cat("i ", i, "index ",index)
# if (index!=-1) {
#   for (index in indexes) {
#     if (data[2,index] == row[1] ) {
#     score = score +1
#     break
#     }
#   }
# }
}
data[2,]
data[593609,]
f = TRUE
!f
